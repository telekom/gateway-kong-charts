<!--
SPDX-FileCopyrightText: 2023-2025 Deutsche Telekom AG

SPDX-License-Identifier: CC0-1.0
-->

# Gateway Helm Chart

This Helm chart deploys the Kong-based API gateway for the [Open Telekom Integration Platform](https://github.com/telekom/Open-Telekom-Integration-Platform).

## Code of Conduct

This project has adopted the [Contributor Covenant](https://www.contributor-covenant.org/) in version 2.1 as our code of conduct. Please see the details in our [CODE_OF_CONDUCT.md](CODE_OF_CONDUCT.md). All contributors must abide by the code of conduct.

By participating in this project, you agree to abide by its [Code of Conduct](./CODE_OF_CONDUCT.md) at all times.

## Licensing

This project follows the [REUSE standard for software licensing](https://reuse.software/).
Each file contains copyright and license information, and license texts can be found in the [./LICENSES](./LICENSES) folder. For more information visit <https://reuse.software/>.

## Installation

### Via OCI Registry

The chart is published to GitHub Container Registry and can be installed directly:

```bash
helm install my-gateway oci://ghcr.io/telekom/o28m-charts/stargate --version 9.0.0 -f my-values.yaml
```

### Via Git Repository

Alternatively, clone the repository and install from source:

```bash
git clone https://github.com/telekom/gateway-kong-charts.git
cd gateway-kong-charts
helm install my-gateway . -f my-values.yaml
```

## Requirements

### Prerequisites

- **Kubernetes Cluster**
- **Helm**: Version 3.x
- **Ingress Controller**: NGINX Ingress Controller (or compatible alternative) for external access

### Container Images

**Important:** Container images are currently not published to a public registry. You must build the images from source and push them to your own registry before installation.

See the [Gateway Repository Overview](https://github.com/telekom/Open-Telekom-Integration-Platform/blob/main/docs/repository_overview.md#gateway) for links to all required component repositories.

### Database

This Gateway requires a PostgreSQL database that will be preconfigured by the Gateway's init container. A PostgreSQL database is deployed automatically with the Gateway by default.

For production use-cases, use an external PostgreSQL database by setting `global.database.location: external` and configuring the `externalDatabase` settings in `values.yaml`.

### Certificate Management

**Manual Secrets (Required):**
You must provide JWT signing key secrets for the Issuer Service and Jumper components. Configure them using `jumper.existingJwkSecretName` and `issuerService.existingJwkSecretName` in `values.yaml`. 

Both components must use identical secrets with the following three-key format:
- `prev-tls.crt`, `prev-tls.key`, `prev-tls.kid` - Previous key (for verifying older tokens)
- `tls.crt`, `tls.key`, `tls.kid` - Current key (for signing new tokens)
- `next-tls.crt`, `next-tls.key`, `next-tls.kid` - Next key (pre-distributed before becoming active)

See the [Automatic Certificate and Key Rotation](#automatic-certificate-and-key-rotation) section for details on the rotation mechanism and secret format.

### Configuration

**Important:** The default `values.yaml` is not ready for deployment out of the box. You must configure the following before installation:
- Image registry and repository settings
- Database passwords (production environments)
- Ingress hostnames and TLS certificates
- Environment-specific labels and metadata
- Resource limits and requests

Refer to the [Open Telekom Integration Platform Documentation](https://github.com/telekom/Open-Telekom-Integration-Platform/blob/main/docs/README.md#open-telekom-integration-platform-documentation) for detailed installation guides, and the [Configuration](#configuration) and [Parameters](#parameters) sections below for detailed configuration options.

## Configuration

### Image Configuration

Images are configured using a cascading defaults system with global and component-specific settings.

**Structure:**
```yaml
global:
  image:
    registry: mtr.devops.telekom.de
    namespace: eu_it_co_development/o28m

image:
  # registry: ""       # Optional: Override global.image.registry
  # namespace: ""      # Optional: Override global.image.namespace
  repository: gateway-kong
  tag: "1.1.0"
```

Images are constructed as: `{registry}/{namespace}/{repository}:{tag}`

Each component (Kong, Jumper, Issuer Service, PostgreSQL, Jobs) can override the global registry and namespace settings individually.

### Database Configuration

PostgreSQL is deployed with the Gateway and requires minimal configuration. **Important:** Change the default passwords!

### External Access

The Gateway can be accessed via Ingress resources. See the [Parameters](#parameters) section for configuration details.

### Security Context

The chart includes hardened security contexts by default that are compliant with most Kubernetes platform requirements.

**Default security contexts:**
- All containers run as non-root users
- Read-only root filesystems where applicable
- All capabilities dropped
- Privilege escalation disabled
- Component-specific user/group IDs

**Customization:**
Customize security contexts per component or globally. See `values.yaml` for all options:

```yaml
securityContext:
  runAsNonRoot: true
  runAsUser: 1000
  # ... additional settings

containerSecurityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  # ... additional settings
```

### Storage and Ingress Classes

**Storage Class:**
By default, the chart uses the cluster's default storage class. Override per component if needed:

```yaml
postgresql:
  persistence:
    storageClassName: "my-storage-class"  # Or "" for cluster default
```

**Ingress Class:**
Ingress class configuration supports cascading defaults:

```yaml
# Set global default for all ingresses
global:
  ingress:
    ingressClassName: nginx

# Or override per component
proxy:
  ingress:
    className: traefik  # Takes precedence over global setting
```

### Health Probes

The Gateway is fully operational only when all components (Kong, Jumper, Issuer Service) are healthy. This is critical for rolling updates.

Each container has its own `readinessProbe`, `livenessProbe`, and `startupProbe` configuration.

**Probe URLs:**

- `http://localhost:8100/status` as readiness probe for Kong
- `http://localhost:8100/status` as liveness probe for Kong
- `http://localhost:8100/status` as startup probe for Kong
- `http://localhost:8080/actuator/health/readiness` as readiness probe for each Jumper container ("jumper")
- `http://localhost:8080/actuator/health/liveness` as liveness probe for each Jumper container ("jumper")
- `http://localhost:8080/actuator/health/liveness` as startup probe for each Jumper container ("jumper")
- `http://localhost:8081/health` as readiness probe for each Issuer-service container
- `http://localhost:8081/health` as liveness probe for each Issuer-service container
- `http://localhost:8081/health` as startup probe for each Issuer-service container

**Configuration:**

Each component has dedicated probe settings in `values.yaml`. Undefined values use Kubernetes defaults.

| Component        | Helm Values                                                                               |
| ---------------- | ----------------------------------------------------------------------------------------- |
| `kong`           | `readinessProbe`, `livenessProbe`, `startupProbe`                                         |
| `jumper`         | `jumper.readinessProbe`, `jumper.livenessProbe`, `jumper.startupProbe`                    |
| `issuer-service` | `issuerService.readinessProbe`, `issuerService.livenessProbe`, `issuerService.startupProbe` |

**Example** (Kong container defaults):

```yaml
livenessProbe:
  httpGet:
    path: /status
    port: status
    scheme: HTTP
  timeoutSeconds: 5
  periodSeconds: 20
  failureThreshold: 4
readinessProbe:
  httpGet:
    path: /status
    port: status
    scheme: HTTP
  timeoutSeconds: 2
startupProbe:
  httpGet:
    path: /status
    port: status
    scheme: HTTP
  initialDelaySeconds: 5
  timeoutSeconds: 1
  periodSeconds: 1
  failureThreshold: 295
```

## Security

### Admin API Protection

**Warning:** Exposing the Admin API for Kong Community Edition is dangerous, as the API is not protected by RBAC and can be accessed by anyone with the API URL. Therefore, the Admin API Ingress is disabled by default.

The Admin API is protected via a dedicated service and route with JWT-Keycloak authentication. Configure the issuer in your values.

### htpasswd Authentication

Create htpasswd for the admin user using the Apache htpasswd tool.

**Prerequisite:** An existing `gatewayAdminApiKey` for the deployment.

1. Generate htpasswd: `htpasswd -cb htpasswd admin gatewayAdminApiKey`
2. Copy the file content into the desired secret (ensure no spaces or line breaks)
3. Verify (recommended): `htpasswd -vb htpasswd admin gatewayAdminApiKey`
4. Deploy and verify setup jobs can access the Kong Admin API and the admin route is accessible manually

### SSL Verification

When SSL verification is enabled, the Gateway verifies all traffic against a bundle of trusted CA certificates.

You can enable SSL verification by setting `sslVerify: true` in `values.yaml`. You must provide your own truststore via the `trustedCaCertificates` field with CA certificates in PEM format, otherwise Kong will fail to start.

**Example:**

```yaml
sslVerify: true
trustedCaCertificates: |
  -----BEGIN CERTIFICATE-----
  <CA certificate 01 in PEM format here>
  -----END CERTIFICATE-----
  -----BEGIN CERTIFICATE-----
  <CA certificate 02 in PEM format here>
  -----END CERTIFICATE-----
  -----BEGIN CERTIFICATE-----
  <CA certificate 03 in PEM format here>
  -----END CERTIFICATE-----
```

Helm let's you reference multiple values files during installation. You could leverage this to externalize `trustedCaCertificates` to a separate values file (e.g., `my-trusted-ca-certificates.yaml`).

### Supported TLS Versions

Supported TLS versions: TLSv1.2 and TLSv1.3. TLSv1.1 is *not* supported.

### Server Certificate

When HTTPS is used without SNI configuration, the API gateway provides a default server certificate for `https://localhost`. Replace this default with a custom certificate by specifying the secret name in `defaultTlsSecret`.

**Example:**

```yaml
defaultTlsSecret: my-https-secret
```

Here are some examples of how to create a custom TLS secret from PEM files. Refer to the [Kubernetes documentation](https://kubernetes.io/docs/concepts/configuration/secret/#tls-secrets) for more details.

```sh
kubectl create secret tls my-https-secret --key=key.pem --cert=cert.pem
oc create secret generic my-https-secret-2 --from-file=tls.key=key.pem  --from-file=tls.crt=cert.pem
```

## Bootstrap and Upgrade

The chart provides specialized jobs for database migrations during initial setup and upgrades.

### Bootstrap

Bootstrapping is required when Kong starts for the first time to set up its database. Set `migrations: bootstrap` in `values.yaml` for new deployments. The bootstrap job is idempotent — running it multiple times is safe.

### Upgrade

Upgrading to a newer version may require database migrations. Set `migrations: upgrade` in `values.yaml` to run pre- and post-upgrade migration jobs.

**Warning:** Remove or comment out `migrations: upgrade` after a successful deployment to prevent re-running migrations.

**Important:** For detailed upgrade instructions, breaking changes, and migration guides, see [UPGRADE.md](UPGRADE.md).

## Advanced Features

The following sections describe advanced configuration options for production deployments.

### Autoscaling

Some environments (especially production workloads) can benefit from autoscaling to automatically adjust workload resources. This chart provides two different autoscaling options: standard HPA and KEDA-based autoscaling.

#### Standard HPA (Horizontal Pod Autoscaler)

See the [Kubernetes HPA documentation](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) for details.

For configuration options, see the `hpaAutoscaling.*` values in the [Parameters](#parameters) section below.

#### KEDA-Based Autoscaling (Advanced)

**Available since chart version `8.0.0`**

[KEDA (Kubernetes Event-Driven Autoscaling)](https://keda.sh/) provides advanced autoscaling capabilities beyond standard HPA, including:
- **Multiple metric sources**: CPU, memory, custom metrics from Victoria Metrics, and time-based schedules
- **Anti-flapping protection**: Sophisticated cooldown periods and stabilization windows
- **Custom metrics**: Scale based on request rate, error rate, or any Prometheus/Victoria Metrics query
- **Schedule-based scaling**: Pre-scale for known traffic patterns (business hours, weekends, etc.)

**Prerequisites:**
- KEDA must be installed in the cluster: `helm install keda kedacore/keda --namespace keda --create-namespace`
- Kubernetes metrics server must be running (for CPU/memory scaling)
- Victoria Metrics must be accessible (for custom metric scaling)
- TriggerAuthentication or ClusterTriggerAuthentication resource must exist (for Victoria Metrics auth)

**Important:** `kedaAutoscaling` and `hpaAutoscaling` are mutually exclusive. Enable only one.

**Minimal Configuration** (CPU and memory only):

```yaml
kedaAutoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  triggers:
    cpu:
      enabled: true
      threshold: 70  # Scale up when CPU > 70%
    memory:
      enabled: true
      threshold: 85  # Scale up when memory > 85%
    prometheus:
      enabled: false
    cron:
      enabled: false
```

For configuration options, see the `kedaAutoscaling.*` values in the [Parameters](#parameters) section below.

**References:**
- [KEDA Documentation](https://keda.sh/docs/)
- [KEDA Scalers](https://keda.sh/docs/scalers/)
- [Victoria Metrics PromQL](https://docs.victoriametrics.com/MetricsQL.html)

### Argo Rollouts (Progressive Delivery)

**Available since chart version `8.1.0` — BETA**

**Note:** The Helm values API is in early stages. Values and templates may change in future versions.

[Argo Rollouts](https://argoproj.github.io/rollouts/) provides advanced deployment capabilities with progressive delivery strategies like canary and blue-green deployments. When enabled, Argo Rollouts manages the rollout process while maintaining the existing Deployment resource through `workloadRef`.

**Features:**
- **Canary Deployments**: Gradually shift traffic from stable to new version with configurable steps
- **Blue-Green Deployments**: Run two identical production environments (blue and green)
- **Automated Analysis**: Optional metric-based validation using Prometheus/Victoria Metrics
- **Traffic Management**: NGINX Ingress-based traffic splitting with canary annotations
- **Automated Rollbacks**: Automatic rollback based on analysis metrics (error rate, success rate)

**Prerequisites:**
- Argo Rollouts must be installed in the cluster: `kubectl create namespace argo-rollouts && kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml`
- NGINX Ingress Controller (for traffic routing)
- Prometheus/Victoria Metrics (optional, for automated analysis)

**Important:** `argoRollouts` and `hpaAutoscaling` (HPA) are mutually exclusive. KEDA autoscaling can be used together with Argo Rollouts.

**Initial Takeover from Existing Deployment**

When enabled, Argo Rollouts manages the rollout process while maintaining the existing Deployment resource through `workloadRef`. It controls scaling up new ReplicaSets and scaling down old ones. This creates a situation where the old Deployment goes out of sync with Argo CD, which attempts to scale up the old Deployment again.

Normally, the Helm chart does not render the replica field. However, during initial takeover (especially when using autoscaling), you must explicitly set replicas to 0 by configuring `argoRollouts.workloadRef.explicitDownscale=true`. After the first migration to Argo Rollouts, remove this property (it defaults to false) to resume normal operation.

**Minimal Configuration** (canary without analysis):

```yaml
argoRollouts:
  enabled: true
  
  strategy:
    type: canary
    canary:
      steps:
        - setWeight: 10
        - pause:
            duration: 2m
        - setWeight: 50
        - pause:
            duration: 5m
```

**Advanced Configuration** (canary with automated analysis):

```yaml
argoRollouts:
  enabled: true
  
  strategy:
    type: canary
    canary:
      additionalProperties:
        maxUnavailable: "50%"
        maxSurge: "25%"
        dynamicStableScale: true
      
      steps:
        - setWeight: 10
        - pause:
            duration: 2m
        - setWeight: 50
        - pause:
            duration: 5m
      
      analysis:
        templates:
          - templateName: success-rate-analysis
  
  analysisTemplates:
    enabled: true
    
    successRate:
      enabled: true
      interval: 30s
      count: 0
      failureLimit: 3
      successCondition: "all(result, # >= 0.95)"
      prometheusAddress: "http://prometheus.monitoring.svc.cluster.local:8427"
      authentication:
        enabled: true
        secretName: "victoria-metrics-secret"
        basicKey: "basic-auth"
```

**Blue-Green Deployment:**

```yaml
argoRollouts:
  enabled: true
  
  strategy:
    type: blueGreen
    blueGreen:
      autoPromotionEnabled: false    # Manual promotion required
      scaleDownDelaySeconds: 30      # Wait before scaling down old version
      prePromotionAnalysis:          # Optional: analysis before promotion
        templates:
          - templateName: success-rate-analysis
```

For configuration options, see the `argoRollouts.*` values in the [Parameters](#parameters) section below.

For detailed configuration, examples, and troubleshooting, see the [Argo Rollouts Feature Documentation](docs/ARGO_ROLLOUTS_FEATURE.md).

**References:**
- [Argo Rollouts Documentation](https://argoproj.github.io/rollouts/)
- [Argo Rollouts Canary Strategy](https://argoproj.github.io/rollouts/features/canary/)
- [Argo Rollouts Analysis](https://argoproj.github.io/rollouts/features/analysis/)

### Pod Anti-Affinity and Topology Key

Distribute pods across multiple nodes for high availability. If one node fails, pods on other nodes continue serving traffic.

Configure pod distribution using the `topologyKey` setting. See the [Parameters](#parameters) section for configuration options and the [Kubernetes documentation](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity) for details on pod anti-affinity.

### Automatic Certificate and Key Rotation

**Available since chart version `6.0.0`**

The Gateway supports automatic rotation of JWT signing keys and certificates used by the Issuer Service and Jumper components. This feature provides zero-downtime key rotation with graceful transitions.

**How It Works:**

The rotation system uses a three-key approach:
- **prev-tls.*** - Previous key (for verifying older tokens)
- **tls.*** - Current key (for signing new tokens)
- **next-tls.*** - Next key (pre-distributed before becoming active)

When certificates are renewed:
1. Current key moves to previous (`tls.*` → `prev-tls.*`)
2. Next key becomes current (`next-tls.*` → `tls.*`)
3. New certificate becomes next (source → `next-tls.*`)

This ensures:
- Resource servers can verify tokens signed with the previous key
- The next key is pre-distributed before activation
- Smooth rotation despite eventual consistency in volume mount propagation

**Prerequisites:**
- [cert-manager](https://cert-manager.io/) installed and configured
- [gateway-rotator](https://github.com/telekom/gateway-rotator) operator deployed

**Configuration:**

Enable automatic rotation by setting `keyRotation.enabled=true` in `values.yaml`:

```yaml
keyRotation:
  enabled: true
```

This deploys the necessary Certificate resource that cert-manager will manage. The gateway-rotator operator watches for certificate renewals and maintains the three-key rotation pattern automatically.

For a more detailed description of the rotation mechanism, see the [gateway-rotator documentation](https://github.com/telekom/gateway-rotator).

**Manual Secret Management:**

Alternatively, provide your own secrets:

```yaml
jumper:
  existingJwkSecretName: my-custom-jwk-secret

issuerService:
  existingJwkSecretName: my-custom-jwk-secret
```

**Important:** Both components must use identical secrets. The secret must conform to the three-key format with `prev-tls.*`, `tls.*`, and `next-tls.*` fields.

**References:**
- [cert-manager Documentation](https://cert-manager.io/docs/)
- [gateway-rotator Key Rotation Process](https://github.com/telekom/gateway-rotator#key-rotation-process)

### Kong Latency Tuning

Large updates via the Admin API can cause latency in the Gateway runtime. This is related to [Kong issue #7543](https://github.com/Kong/kong/issues/7543).

You can tune Kong's asynchronous route refresh behavior with these variables:

| Helm-Chart variable        | Kong property                      | default value | documentation link              |
| -------------------------- | ---------------------------------- | ------------- | ------------------------------- |
| nginxWorkerProcesses       | KONG_NGINX_WORKER_PROCESSES        | 4             |                                 |
| workerConsistency          | KONG_WORKER_CONSISTENCY            | eventual      | [worker_consistency]            |
| workerStateUpdateFrequency | KONG_DB_UPDATE_FREQUENCY           | 10            | [db_update_frequency]           |
| dbUpdatePropagation        | KONG_DB_UPDATE_PROPAGATION         | 0             | [db_update_propagation]         |
| dbUpdateFrequency          | KONG_WORKER_STATE_UPDATE_FREQUENCY | 10            | [worker_state_update_frequency] |

[worker_consistency]: https://docs.konghq.com/gateway/2.8.x/reference/configuration/#worker_consistency
[db_update_frequency]: https://docs.konghq.com/gateway/2.8.x/reference/configuration/#db_update_frequency
[db_update_propagation]: https://docs.konghq.com/gateway/2.8.x/reference/configuration/#db_update_propagation
[worker_state_update_frequency]: https://docs.konghq.com/gateway/2.8.x/reference/configuration/#worker_state_update_frequency

### Relabeling application metrics

In case you need to manipulate existing application metrcis or add new ones,
you can use the `metricRelableings` for both ServiceMonitor and PodMonitor resources.
An example that adds a new label would look like this:

```yaml
prometheus:
  enabled: true
  [...]

  podMonitor:
    enabled: true
    metricRelabelings:
      - action: replace
        targetLabel: example-label-key
        replacement: example-label-value
    [...]
```

## Parameters

The following table provides a comprehensive list of all configurable parameters in `values.yaml`:

{{ template "chart.valuesSection" . }}

## Troubleshooting

If the Gateway deployment fails to start, check the container logs for error messages.

### SSL Verification Error

**Symptom:**

```
Error: /usr/local/share/lua/5.1/opt/kong/cmd/start.lua:37: nginx configuration is invalid (exit code 1):
nginx: [emerg] SSL_CTX_load_verify_locations("/usr/local/opt/kong/tif/trusted-ca-certificates.pem") failed (SSL: error:0B084088:x509 certificate routines:X509_load_cert_crl_file:no certificate or crl found)
nginx: configuration file /opt/kong/nginx.conf test failed
```

**Solution:**
This error occurs when `sslVerify` is set to `true` but no valid certificates are provided. Either:
- Set `trustedCaCertificates` with valid CA certificates in PEM format
- Set `sslVerify: false` if SSL verification is not required
